# Therapist-and-Child-Engagement-Analysis-for-Autism-detection

Objective: Develop an optimized inference pipeline to analyze therapist and child engagement in a video using gaze, emotion, and object interaction prediction models.

Approach:

Leveraged pre-trained models:
Gaze estimation model (replace with specific model used)
Emotion recognition model (replace with specific model used)
SAM (Segment Anything Model) for object detection
Implemented core functionalities:
Preprocessing video for model compatibility (resizing, normalization).
Individual prediction functions for gaze, emotion, and object interaction.
Optional: Visualization logic to overlay predictions on video frames.
Optional: Storing prediction results for further analysis.
Key Contributions:

Integrated SAM for object detection, enabling identification of objects relevant to child-therapist interaction.
Addressed missing elements in the initial code snippet to provide a more comprehensive pipeline.
Ethical Considerations:

Acknowledged limitations of current models, especially gaze estimation with children.
Emphasized the importance of using additional methods like eye tracking for more precise analysis.






 You can run the model but  before that there is some dependency .
 1- Clone the repository recursively:
git clone --recurse-submodules https://github.com/nizhf/hoi-prediction-gaze-transformer.git

 you can directly  use the code files .

 
